{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e62976-c1ab-41cd-8bb8-8dc4e7bbc4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class IndexFileCreator:\n",
    "    def __init__(self, crawled_pages_folder):\n",
    "        self.crawled_pages_folder = crawled_pages_folder\n",
    "        self.inverted_index = {}\n",
    "\n",
    "    def generate_meta_file(self, meta_file_path):\n",
    "        with open(meta_file_path, 'w', encoding='utf-8') as meta_file:\n",
    "            for file_name in os.listdir(self.crawled_pages_folder):\n",
    "                file_path = os.path.join(self.crawled_pages_folder, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    with open(file_path, 'r', encoding='utf-8') as page_file:\n",
    "                        content = page_file.read()\n",
    "                        meta_info = self.extract_meta_info(content, file_name)  \n",
    "                        meta_file.write(f\"Web Page ID: {meta_info['page_id']}\\n\")\n",
    "                        meta_file.write(f\"URI: {meta_info['url']}\\n\")\n",
    "                        meta_file.write(f\"Title: {meta_info['title']}\\n\")\n",
    "                        meta_file.write(f\"Page Body: {meta_info['body']}\\n\")\n",
    "                        meta_file.write(f\"Description Keywords: {', '.join(meta_info['keywords'])}\\n\\n\")\n",
    "\n",
    "    def extract_meta_info(self, content, file_name):\n",
    "        url_pattern = re.compile(r\"URL: (.+)\")\n",
    "        title_pattern = re.compile(r\"Title: (.+)\")\n",
    "        body_pattern = re.compile(r\"Page Body:(.+?)(?=Headings:|$)\", re.DOTALL)\n",
    "        heading_pattern = re.compile(r\"Headings:(.+?)Page Body:\", re.DOTALL)\n",
    "    \n",
    "        url_match = url_pattern.search(content)\n",
    "        title_match = title_pattern.search(content)\n",
    "        body_match = body_pattern.search(content)\n",
    "        heading_match = heading_pattern.search(content)\n",
    "    \n",
    "        page_id_match = re.search(r'page(\\d+)\\.txt', file_name)\n",
    "        page_id = int(page_id_match.group(1)) if page_id_match else None\n",
    "    \n",
    "        url = url_match.group(1).strip() if url_match else \"\"\n",
    "        title = title_match.group(1).strip() if title_match else \"\"\n",
    "        body_text = body_match.group(1).strip() if body_match else \"\"\n",
    "        body = ' '.join(body_text.split()[:200])\n",
    "        \n",
    "        headings = heading_match.group(1).strip().split(\"\\n-\") if heading_match else []\n",
    "    \n",
    "        # Combine body text with headings for keyword extraction\n",
    "        all_text = \" \".join(headings) + \" \" + body_text\n",
    "        \n",
    "    \n",
    "        # Tokenize and normalize text\n",
    "        tokens = word_tokenize(all_text.lower())\n",
    "        tokens = [token for token in tokens if token.isalnum()]\n",
    "    \n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "        # Stemming\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stemmed_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "        return {'page_id': page_id, 'url': url, 'title': title, 'keywords': stemmed_tokens,'body':body}\n",
    "\n",
    "    def generate_inverted_index(self, inverted_index_file_path):\n",
    "        for file_name in os.listdir(self.crawled_pages_folder):\n",
    "            file_path = os.path.join(self.crawled_pages_folder, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as page_file:\n",
    "                    content = page_file.read()\n",
    "                    meta_info = self.extract_meta_info(content, file_name)\n",
    "                    for keyword in meta_info['keywords']:\n",
    "                        if keyword not in self.inverted_index:\n",
    "                            self.inverted_index[keyword] = {'page_ids': set(), 'frequency': 0}\n",
    "                        self.inverted_index[keyword]['page_ids'].add(meta_info['page_id'])\n",
    "                        self.inverted_index[keyword]['frequency'] += 1\n",
    "    \n",
    "        with open(inverted_index_file_path, 'w', encoding='utf-8') as inverted_index_file:\n",
    "            inverted_index_file.write(\"Keyword|Frequency|Document\\n\")\n",
    "            for keyword, data in self.inverted_index.items():\n",
    "                page_ids = sorted(data['page_ids'])\n",
    "                frequency = data['frequency']\n",
    "                inverted_index_file.write(f\"{keyword}|{frequency}| {' '.join(map(str, page_ids))}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c8d01c9-cb22-46d0-91d9-23a2e5d24a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta file and inverted index file have been generated successfully.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define the path to the folder containing crawled pages\n",
    "    crawled_pages_folder = \"crawled_pages\"\n",
    "    \n",
    "    # Create an instance of MetaFileCreator\n",
    "    meta_creator = IndexFileCreator(crawled_pages_folder)\n",
    "    \n",
    "    # Define the path where you want to save the meta file\n",
    "    meta_file_path = \"meta.txt\"\n",
    "    \n",
    "    # Generate the meta file\n",
    "    meta_creator.generate_meta_file(meta_file_path)\n",
    "    \n",
    "    # Define the path where you want to save the inverted index file\n",
    "    inverted_index_file_path = \"inverted_index.txt\"\n",
    "    \n",
    "    # Generate the inverted index file\n",
    "    meta_creator.generate_inverted_index(inverted_index_file_path)\n",
    "    \n",
    "    # Print a message indicating both files have been generated\n",
    "    print(\"Meta file and inverted index file have been generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ea556-035f-405b-8c5a-abd833184aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
